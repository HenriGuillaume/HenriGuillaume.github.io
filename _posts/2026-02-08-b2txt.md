---
layout: post
title: "A functional syllable-based neuroprosthesis with OT-based alignment"
date: 2025-09-12
---

<head>
    <!-- Other head elements -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']]
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

</head>

This post serves as a report about an extension of the work I did during my internship at [Institut de l'Audition](https://www.institut-audition.fr/). It also serves as a solution to the [Brain-to-text '25 data challenge](https://www.kaggle.com/competitions/brain-to-text-25), although no finished solution was formally uploaded there.

**All of the code described in this post can be found [here](https://github.com/HenriGuillaume/b2txt2025)**

# "Abstract"
The last decade has seen great improvements in the decoding of neural speech, mainly thanks to numerous advances in machine learning technologies. Syllables make a natural target for this task, as they seem to constitute a base unit of language during speech production <sup>[1](#giraud)</sup>. <br>
However, the sheer number of unique syllables in most languages makes this approach intractable for machine learning applications <sup>[2](#many_syllables)</sup> To circumvent this label diversity problem, we can use a new approach by predicting the continuous state-space embeddings of Sylber <sup>[3](#Sylber)</sup>, a syllable-based audio deep learning model, and apply it to the Brain to text ‘25 challenge.<br>
Furthermore, we learn these features with no ground truth alignment, by using a new optimal transport - based training loss <sup>[4](#SOTD)</sup>

# Data
Information about the dataset can be found on the [Brain-to-text '25 Kaggle](https://www.kaggle.com/competitions/brain-to-text-25).
Our main learning objective consists of features generated by the Sylber<sup>[3](#Sylber)</sup> model. This model takes in audio and outputs features that can (roughly) be viewed as continuous state-space embeddings of syllables. 
However, the ground truth in the dataset in purely textual. To obtain Sylber features, the ground truth sentence labels were fed into PiperTTs<sup>[7](#Piper)</sup>, a text-to-speech model that generates artificial audio that was in turn fed into the Sylber model.

# Training method: SOTD
The dataset provides no ground-truth alignment between the neural signal and the corresponding sentence/phoneme sequence, this makes the challenge highly transferable to the question of imagined speech recognition (which is the end goal for patients with locked-in syndrome).
This historically has been tackled with CTC, however, since our targets live in continuous state-space, we cannot rely on this.
Note that [some of the challenges solutions](https://www.kaggle.com/code/jamalsaeedi/b2t-infer) implement end to end learning paradigms that do not require explicit alignment.
Sequence Optimal Transport Distance (SOTD) is a novel method that allows learning of alignment between sequences. It is highly similar to DTW, with the added feature of an annex model that generates a distribution over the predicted sequence, giving more weight to certain time frames.<br>
It is worth noting that there is little to none probabilistic interpretation and that the "optimal transport" framework serves as more of a pretext than anything. The loss for two sequences $$\{x_i\}_{i \leq n}$$ and $$\{y_j\}_{j \leq m}$$ is expressed as:<br>
<center>
$$
\mathcal{L}_{OTTR} = -\sum_{i,j=1}^{n,m}\gamma_{n}^{m, \beta_m}(\alpha[{x}_n], W)_{i,j} \cdot ||x_i - y_j||_2^2
$$
</center>

With $$\gamma$$ being the solution to the monotonous Wasserstein problem (_c.f._ the SOTD paper<sup>[4](#SOTD)</sup>).

This induces an $$O(nm)$$ complexity which will turn out to be a problem for our computer.

<div style="text-align:center;">
  <div style="display:flex; justify-content:center; gap:10px;">
    <img src="/assets/b2txt/gifs/loop0.gif"     width="250">
    <img src="/assets/b2txt/gifs/loop100.gif"   width="250">
    <img src="/assets/b2txt/gifs/loop1000.gif"  width="250">
    <img src="/assets/b2txt/gifs/loop1423.gif"  width="250">
  </div>
  <p style="font-style: italic; color: #888;">
    From left to right: Iteration 0, 100, 1000, 1423 (a hard one). Shows the $\alpha$ distribution (top), the prediction Gram matrix (above), the ground truth Gram matrix (below), as well as the transport plan between the two (darker lines represent higher cost)
  </p>
</div>


# Model
All training was performed on a single consumer-grade _NVIDIA RTX 2070 SUPER_ GPU. This greatly limits performance, and forced us to heavily temporally downsample the sequence in order to compute the SOTD loss.
The model consists of a simple convnet followed by a conformer block.<br>



<img src="/assets/b2txt/images/model.png" alt="My Cat" width="300">



# Results
The accuracy is rather bad, between 60-80%, we didn't bother getting proper measures as we do not have enough compute to train a satisfying enough model.
The advantage of this approach is that it allows for direct vocal reconstruction of the syllables, potentially with the patient's voice (see the Sylber paper). We can also note that when inaccurate, the syllables will be phonetically close to what they should be, another interesting feature of Sylber's embedding.

## Reconstruction from predicting later levels of the architecture does not necessarily yield better results
L. Evanson et al. <sup>[6](#Emergence)</sup> showed that, through a simple linear model, neural activity from speech perception can be used to predict activations in audio and language models. Instead of solely predicting the output of the Sylber model, corresponding to interpretable syllables, we can try to train different models to predict different layers of the architecture and forward through the remaining layers from there.

<img src="/assets/b2txt/images/layer_predictions.png" alt="My Cat" width="600">

To do so, we use the same simple 5-layer "patched" GRU as <sup>[5](#Willett)</sup> for each layer. We then reconstruct the audio from the forwarded features.
Surprisingly, although predicting later features proved overall more accurate, there were instances in which better results were obtained from earlier layers. This suggests that some earlier layers of Sylber may encode non-syllabic information that can outperform syllables in certain cases.


## Target sentence: _"A lot of people complain"_

| Predicted Layer   | Audio reconstruction |
|-------------------|----------------------|
| Layer 0 | <audio controls><source src="/assets/b2txt/audio/layer0.wav" type="audio/mpeg">Your browser does not support the audio element.</audio> |
| Layer 1 | <audio controls><source src="/assets/b2txt/audio/layer1.wav" type="audio/mpeg">Your browser does not support the audio element.</audio> |
| Layer 2 | <audio controls><source src="/assets/b2txt/audio/layer2.wav" type="audio/mpeg">Your browser does not support the audio element.</audio> |
| Layer 3 | <audio controls><source src="/assets/b2txt/audio/layer3.wav" type="audio/mpeg">Your browser does not support the audio element.</audio> |
| Layer 4 | <audio controls><source src="/assets/b2txt/audio/layer4.wav" type="audio/mpeg">Your browser does not support the audio element.</audio> |
| Layer 5 **(best)** | <audio controls><source src="/assets/b2txt/audio/layer5.wav" type="audio/mpeg">Your browser does not support the audio element.</audio> |
| Layer 6 | <audio controls><source src="/assets/b2txt/audio/layer6.wav" type="audio/mpeg">Your browser does not support the audio element.</audio> |
| Layer 7 | <audio controls><source src="/assets/b2txt/audio/layer7.wav" type="audio/mpeg">Your browser does not support the audio element.</audio> |


# Roadmap
There are many things I would want to experiment with if I had the time and resources, below is a list of to-do's/questions for your new intern/claude code/whoever wants to do it

- Try a network that doesn't temporally downsample so much. 
- Reduce the dimension of Sylber's features, which are currently 768-dimensional. Trying to reconstruct the features with a simple MLP with a dimension bottleneck in the middle, autoencoder-style should work.  

# References

<a id="giraud"></a>
[1] A. Giraud and D. Poeppel, “Cortical oscillations and speech processing: emerging computational principles and operations,” Nature Neuroscience, vol. 15, no. 4, pp. 511–517, 2012, doi: 10.1038/nn.3063.

<a id="many_syllables"></a>
[2] Y. M. Oh, “Linguistic Complexity and Information: Quantitative Approaches,” 2015. \[Online]. Available: http://www.ddl.cnrs.fr/fulltext/Yoonmi/Oh_2015_1.pdf

<a id="Sylber"></a>
[3] C. J. Cho et al., “Sylber: Syllabic Embedding Representation of Speech from Raw Audio,” arXiv preprint arXiv:2410.07168, 2024, [Online]. Available: https://arxiv.org/abs/2410.07168

<a id="SOTD"></a>
[4] Y. Kaloga, S. Kumar, P. Motlicek, and I. Kodrasi, “A Differentiable Alignment Framework for Sequence-to-Sequence Modeling via Optimal Transport,” arXiv preprint arXiv:2502.01588, Feb. 2025.

<a id="Willett"></a>
[5] F. R. Willett et al., “A high‑performance speech neuroprosthesis,” Nature, vol. 620, no.7976, pp. 1031–1036, 2023, doi: 10.1038/s41586-023-06377-x.

<a id="Emergence"></a>
[6] L. Evanson et al., “Emergence of Language in the Developing Brain,” Unpublished Manuscript, 2025.

<a id="Piper"></a>
[7] OHF‑Voice, “piper1‑gpl: Fast and local neural text‑to‑speech engine.” 2025.
