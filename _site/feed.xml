<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-08T23:32:37+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Henri Guillaume</title><subtitle>Maths</subtitle><entry><title type="html">Optimal group testing</title><link href="http://localhost:4000/2024/11/08/group_testing.html" rel="alternate" type="text/html" title="Optimal group testing" /><published>2024-11-08T00:00:00+01:00</published><updated>2024-11-08T00:00:00+01:00</updated><id>http://localhost:4000/2024/11/08/group_testing</id><content type="html" xml:base="http://localhost:4000/2024/11/08/group_testing.html"><![CDATA[<head>
    <!-- Other head elements -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<h1 id="optimal-group-testing">OPTIMAL GROUP TESTING</h1>

<p>This notebook is an intereactive explanation of the paper “Optimal Group Testing” (2020) by Amin Coja-Oghlan, Oliver Gebhard, Max Hahn-Klimroth and Philipp Loick. The paper can be found <a href="https://proceedings.mlr.press/v125/coja-oghlan20a.html">here</a>.<br />
All credit is evidently due to them, this is merely a class project.</p>

<h2 id="a-little-context">A little context</h2>
<p>Let’s say that you’re a military doctor, charged with testing your whole army for syphilis. Unfortunately, you need to pay, say 50$ per sample of blood that you test, that could quickly get expensive!<br />
However, your name is Robert DORFMAN, and you get the idea that you can get the same amount of information without testing each soldier individually, the idea is the following:<br />
You mix the blood that you drew from your soldiers in <em>groups</em>, and you test the mixed blood samples for syphilis, so that a <span style="color: red;">positive</span> result tells you that <em>at least one</em> soldier in the group is infected.<br />
Now imagine that you get really lucky, and that a pool of blood from 5 soldiers tests <span style="color: green;">negative,</span> you know that none of them are infected, and you effectively saved 4 tests! (that’s 200$ you can spend on the toyota supra you’ve been saving for!)</p>

<h2 id="problem-setup">Problem setup</h2>

<p>We are given a population of size \(n\), and a number \(k\) of infected or “defective” individuals. (Note that the term “defective” isn’t meant to be reductive of ill people, group testing has applications in manufacturing of certain components).<br />
We assume that
\(k \sim n^\theta\)
meaning that the number of infected \(k\) is asymptotically proportional to \(n^\theta\), for a certain \(\theta \in (0,1)\) i.e. proportional for a large population.<br />
You might ask the question:</p>

<center>"Why is the proportion written as an exponent, instead of simply being a multiplicative constant?"</center>

<p><br />
This has to do with the fact that expressing the number of infected as \(n^\theta\) makes it grow <em>sublinearly</em> with population size, which is more realistic in more use cases.</p>

<p>The whole point of what is coming is to answer the most basic classification problem in medicine:</p>
<center><font size="6"> Who's sick? </font></center>

<p>In more precise terms, given our \(n\) individuals \(\{x_1,..., x_n \}\), we have an unknown vector comprised of \(n-k\) zero-entries for healthy patients, and \(k\) one-entries for sick patients. We will denote this vector \(\sigma \in \{0,1\}^n\). When we say a vector of Hamming weight \(k\), we simply mean that there are \(k\) non-zero entries.<br />
This gives us two goals:</p>
<ul>
  <li>Get an estimator \(\hat{\sigma}\) of \(\sigma\) that suceeds with high probability (<em>w.h.p.</em>)</li>
  <li>Minimise the number of tests \(m\), and importantly find out how low we can make it</li>
</ul>

<p>There are two types of group testing:</p>

<h3 id="non-adaptive-group-testing">Non adaptive group testing:</h3>
<p>In this variant of the problem, we only have the right to one round of tests. This problem is relevant in practical applications as test results may take time to come, so doing \(R\) rounds of testing multiplies your waiting time by \(R\).</p>

<h3 id="adaptive-group-testing">Adaptive group testing:</h3>
<p>In this variant, we allow ourselves to do a first round, wait for the results, and then adapt our strategy to the acquired results. This is obviously advantageous over non adaptive testing as our choice of groups can be made more precise thanks to additional information.</p>

<h2 id="first-ideas">First ideas</h2>

<h3 id="the-random-bipartite-design-definitive-defectives">The random bipartite design, Definitive Defectives</h3>
<p>In this test design, the individuals \(V = \{x_1, ..., x_n\}\) partake in \(\Delta\) tests, chosen uniformly at random among our \(m\) tests \(F = \{a_1, ..., a_m\}\). This is represented by a bipartite graph. This design is based upon two simple ideas:</p>
<ol>
  <li>Any patient \(x\) that appears in a *<span style="color: green;">negative*</span> test \(a\) is <span style="color: green;">negative
2</span>. If a patient \(x\) appears in a <span style="color: red;">positive</span> test \(a\), and all other participants in \(a\) have been marked as uninfected during step 1, then \(x\) is <span style="color: red;">positive</span>. This is because x being postitive is the “only explaination” for \(a\) being <span style="color: red;">positive</span>.</li>
</ol>

<div style="text-align: center;">
    <video src="/assets/SPIV/DD_total.mp4" style="width: 100%;" controls="" autoplay="" muted="" loop=""></video>
    <p style="font-style: italic; color: #555; max-width: 100%; margin-top: 10px;">A very simple example of inference using definite defectives, diagnosing 4 patients with only 3 tests.</p>
</div>

<p>Obviously, this comes with a few caveats. As you can see, the fact that the test above works relies heavily on the fact that all three healthy patients join a test that the infected patient doesnt.<br />
Therefore, if we have too few tests, too big a population, or a great proportion of infected, the algorithm fails. This is because we are at a greater risk of infected patients “poisoning the well” by making our preciously informative <span style="color: green;">negative tests</span> <span style="color: red;">positive</span>.</p>

<div style="text-align: center;">
    <video src="/assets/SPIV/DD_limitations.mp4" style="width: 100%;" controls="" autoplay="" muted="" loop=""></video>
    <p style="font-style: italic; color: #555; max-width: 100%; margin-top: 10px;">"Poisoning the well", by reducing the number of tests and having \(x_1\) make both of our remaning tests <span style="color: red;">positive</span>, therefore containing very little information.</p>
</div>

<p>It is a good idea to pause and ponder about the functioning of definite defectives before moving on to the next part.</p>

<h2 id="spiv">SPIV</h2>

<p>This algorithm is the main topic if this text. SPIV, or <strong>Spatial Inference Vertex Cover</strong> is an non-adaptive algorithm that allows us to infer \(\sigma\) w.h.p. with a minimum number of tests \(m_{SPIV}\) that is asymptotically better than the number \(m_{DD}\) that definite defective requires. Please not that the key word here is <strong>asymptotically</strong>. We will come back to this later.<br />
In the following, we will discuss and try to illsutrate the different stages SPIV, as described in the paper.<br />
I will ask you to not get frightened or stuck on the mathematical details, and try to focus on the ideas of the algortihm.</p>
<h3 id="a-little-intuition">A little intuition</h3>
<p>Let’s start with what I think are good insights to motivate this design.</p>
<ol>
  <li><strong>It makes sense to diagnose <em>locally</em>:</strong> An algorithm like definite defectives connects anyone to anyone else at random. When dealing with a very large population, this doesn’t seem very realistic, and one might be tempted to “divide and conquer” the problem.</li>
  <li><strong>The problem <em>isn’t linear in nature</em>:</strong> With the previous insight in mind, you might think something along the lines of “let’s divide our population of size \(n\) into 5 groups then!”. However, as \(n\) grows, you will obvioulsy encounter the same problem as before, so your denominator has to depend on \(n\). It also has to grow slower than \(n\) (why?), \(\log(n)\) is a rather natural choice.</li>
  <li><strong>It makes sense to <em>link compartments</em>:</strong> Imagine you have divided all of the individuals in one of your subgroups. Having this subgroup cut off from the rest of the population would be a waste of information, hence the idea of linking them.</li>
</ol>

<p><em>Nota bene</em>: when we say that tests (respectively patients) are “linked”, we simply mean that they share patients (respectively tests) in common.</p>

<h3 id="phase-0-setting-up-the-ring">Phase 0: Setting up the ring</h3>
<p>As the title says, the algorithm relies on a <em>ring structure</em>. No algebra there, it simply means that we divide our patients and our tests into compartments and that patients share tests with other patients from following compartments, hence the last compartment shares tests with the first compartments (so that it is also linked), and we obtain a ring.</p>

<div style="text-align: center;">
    <video src="/assets/SPIV/ring_creation.mp4" style="width: 100%;" controls="" autoplay="" muted="" loop=""></video>
    <p style="font-style: italic; color: #555; max-width: 100%; margin-top: 10px;">The SPIV ring structure, keep in mind that the number of tests, test compartments and the links do not have the value SPIV would give them. In fact, for this few patients, SPIV often gives more tests than patients.</p>
</div>

<p>Here are the constants that we need to determine the ring:</p>
<ul>
  <li>\(\mathbf{l}\): The number of compartments our patients and tests are divided into (leaving population compartments of size \(\frac{n}{l}\))</li>
  <li>\(\mathbf{F[1], ... ,F[l]}\): The test compartments (partition of \(\{a_1, ... , a_{m_{SPIV}}\}\))</li>
  <li>\(\mathbf{V[1], ... ,V[l]}\): The patient compartments (partition of \(\{x_1, ... , x_{n}\}\))</li>
  <li>\(\mathbf{s, \Delta}\): A patient in \(F[i]\)  can (and must) choose \(\frac{\Delta}{s}\) tests in each of the compartments \(F[i+1], ..., F[i+s-1]\). These tests are chosen uniformly at random <em>with replacement</em>. This means a patient can join the same test several time. More on that later.</li>
  <li>\(\mathbf{m, m_{add}}\): The number of tests and <em>additional tests</em>, we will explain the reason for these later. This gives us compartments of size \(\frac{m}{l}\) and one additional compartment \(\mathbf{F[0]}\) of additional tests.</li>
</ul>

<p>From there, we will try and build this algorithm by “<a href="https://en.m.wikipedia.org/wiki/Mathematical_induction">induction</a>”. That is we will answer the two questions:</p>
<ol>
  <li>How can we use the information we have about a compartment to better diagnose other compartments? (heredity)</li>
  <li>How do we diagnose the first compartment to start the process? (pushing the first domino, <em>a.k.a.</em> initialization)</li>
</ol>

<h3 id="heredity">Heredity</h3>
<p>Suppose that we have diagnosed all compartments \(V[1], ..., V[i]\), how do we diagnose \(V[i+1]\)?<br />
As earlier, patients who have taken a <span style="color: green;">negative test</span>, must be <span style="color: green;">negative.</span> We cannot say anything more about uninfected patients.<br />
Our next task is to differentiate between <span style="color: red;">positive</span> and <span style="color: green;">negative patients</span> that partook in a <span style="color: red;">positive</span> test, respectively true postives and false postitives. The paper offers us a way to do so, that goes as follows: remember how we diagnosed <span style="color: red;">positive</span> patients in DD by finding those who were “the only explaination” for a test being <span style="color: red;">positive</span>? Well the idea here is similar: For a patient \(x\) in \(V[i+1]\), we look in the \(s\) next test compartments (including \(F[i+1]\)), and we count the number of tests that contain \(x\) but “hadn’t been explained up until now”.
Thus we define: \(\mathbf{W_{x,j}}\) as the number of test in \(F[i+j]\) that do not contain an infected individual from \(V[1], ..., V[i]\)</p>

<p><img src="/assets/SPIV/count.png" alt="image" /></p>
<div style="text-align: center;">
    <p style="font-style: italic; color: #555; max-width: 100%; margin-top: 10px;">Computing \(W\) for \(x_0^{i+1}\), we can see that in compartment \(F[i+1]\), it accounts for one test that wasnt explained before. However, in compartment \(F[i+2]\), even though it does belong to a <span style="color: red;">positive</span> test, this test is already explained by \(x_i^2\), and therefore gives us no information about \(x_0^{i+1}\), which could very well be <span style="color: green;">negative.</span></p>
</div>

<p>As illustrated above, you can see that if \(x \in V[i+1]\) often belongs to tests that had no previous explaination, it is likely to be infected. We therefore look at the quantity \(\mathbb{E}[W_{x,j}]\) to infer the state of \(x\).<br />
However, we can (and in fact must) do a little better. Imagine that we are looking at \(F[i+1]\). Because compartments are only linked to tests <em>farther</em> along the ring, and \(V[1], ..., V[i]\) are not responsible for making \(F[i+1]\) <span style="color: red;">positive</span>,  there’s this idea that \(V[i+1]\) is the “last shot” at explaining why \(F[i+1]\) is <span style="color: red;">positive</span>.</p>

<div style="text-align: center;">
    <video src="/assets/SPIV/lastshot.mp4" style="width: 100%;" controls="" autoplay="" muted="" loop=""></video>
    <p style="font-style: italic; color: #555; max-width: 100%; margin-top: 10px;">We can see that \(x_0^{i+1}\) is the only remaining explaination for \(a_0^{i+1}\) being <span style="color: red;">positive</span>. It therefore has to be <span style="color: red;">positive</span>.</p>
</div>

<p>Conversely, if we look at a <span style="color: red;">positive</span> test \(a \in F[i+s]\), even though it is connected to \(x\), the patient making \(a\) postive could still reside in the next compartments \(V[i+2], ..., V[i+s]\).</p>

<div style="text-align: center;">
    <video src="/assets/SPIV/firstshot.mp4" style="width: 100%;" controls="" autoplay="" muted="" loop=""></video>
    <p style="font-style: italic; color: #555; max-width: 100%; margin-top: 10px;">Sure, \(x_0^{i+1}\) could be the reason \(a_0^{i+3}\) is <span style="color: red;">positive</span>, but because we know nothing about the other patients that have taken \(a_0^{i+3}\) (anyone of which could be the culprit), we cannot know if \(x_0^{i+1}\) is responsible.</p>
</div>
<p>This observation leads us to the idea of assigning weights \(\mathbf{w_j}\) to each \(W_{x,j}\), and we define \(W_x^* = \sum_{j=1}^{s-1}w_jW_{x,j}\) We then define a threshold for this value under which we are confident that \(x\) is <span style="color: green;">negative,</span> this is done by optimising the weights \(w_j\) that minimize the probability of false <span style="color: red;">positive</span>s being classified as true <span style="color: red;">positive</span> (see the paper for more detailed math).</p>

<h4 id="heredity-cleanup-phase">Heredity: Cleanup phase</h4>
<p>This step is all about correcting errors commited in the previous phase. For any \(x\) (that is not in the first \(s\) compartments as we will see below), we count the number of tests \(a\) containing \(x\) that do not have any other explaination. If there are enough such tests (more than \(\ln^{\frac{1}{4}}(n)\)), we infer that \(x\) is <span style="color: red;">positive</span>. Otherwise we say that \(x\) is healthy. We repeat this process \(\lceil \ln(n) \rceil\) times. The idea is that we gave each patients enough tests so that a postitive patient should have at least one test that they don’t share with any other <span style="color: red;">positive</span> patients.</p>

<h3 id="initialization">Initialization</h3>
<p>We now know how to diagnose \(V[i+1]\) once we have diagnosed \(V[1], ..., V[i]\), but this requires stqrting somewhere right? Well yes, and there’s really no tricks to getting the algorithm started: the idea is simply to use the “additional” compartment \(F[0]\) that we defined earlier to run definite defectives on the first \(s\) compartments. This ensures that the first \(s\) compartments are diagnose with high probability. We only use it on a small subset of our population, so we still get to keep the lower asymptotic number of tests needed by SPIV. Remember how we said we’d define what we mean by asymptotic earlier? Well now is the time.</p>

<h2 id="discussion">Discussion</h2>
<p>As we said earlier, the minimum number of tests \(m_{SPIV}\) that is asymptotically better than the number \(m_{DD}\) that definite defective requires. Actually, even better than that <em>it is as close as it can possibly get</em>. This means that no matter, the proportion, for \(n\) great enough, we get  \(m_{SPIV} &lt; m_{DD}\), however, it can be wise to look at what we mean by “great enough”. To get a feel for what this convergence rate might look like, I invite you to play around with the graph down below by adjusting the slider for \(n\). Try to figure out how many trials it takes to do better than definite defectives, or even to reach the theoretical limit for the required minimum number of tests.</p>

<div style="text-align: center;">
  <iframe src="https://www.desmos.com/calculator/66bsv1amrv" width="100%" height="500" style="border: 1px solid #ccc" frameborder="0"></iframe>
  <p style="font-style: italic; color: #555; max-width: 100%; margin-top: 10px;">The \(x\)-coordinates correspond to our "proportion" of infected \(\theta\), the red curve is the information-theoretic limit, the green curve is \(m_{DD}\), and the blue curve is \(m_{SPIV}\)</p>
</div>

<p>This is not to undermine the greatness of this algortihm though. The fact that the authors managed to reach the theoretical limit \(m_{inf}\) is beautiful on it’s own. But seeing the convergence rate, the topic is, in my opinion, far from being fully explored.<br />
I hope that you enjoyed reading this post, and that you learnt something from it!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Farkas’ lemma</title><link href="http://localhost:4000/2024/10/31/farkas.html" rel="alternate" type="text/html" title="Farkas’ lemma" /><published>2024-10-31T00:00:00+01:00</published><updated>2024-10-31T00:00:00+01:00</updated><id>http://localhost:4000/2024/10/31/farkas</id><content type="html" xml:base="http://localhost:4000/2024/10/31/farkas.html"><![CDATA[<head>
    <!-- Other head elements -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<!-- CSS for the animations and sliders -->
<style>
    /* Make sliders fit the width of their container */
    input[type="range"] {
        width: 100%; /* Full width */
        margin-top: 10px; /* Add some space above */
    }
    #image-container-1, #image-container-2, #image-container-3 {
        width: 100%; /* Make containers full width */
        text-align: center; /* Center the images */
    }
    img {
        max-width: 100%; /* Responsive images */
        height: auto; /* Maintain aspect ratio */
    }
</style>

<!-- CSS for the theorem boxes-->
<style>
    .theorem {
        border: 2px solid #007BFF;
        border-radius: 5px;
        background-color: #f9f9f9;
        padding: 15px;
        margin: 20px 0;
        font-family: Arial, sans-serif;
    }

    .theorem h4 {
        margin: 0;
        font-weight: bold;
    }
</style>

<p>On the surface, this post is a visual explaination of Farkas’ lemma, which mind you, isn’t a difficult lemma to visualise, but still has a great geometrical interpretation. However, my main goal when writing this was to experiment with <a href="https://www.manim.community/">Manim</a> and creating “interactive” animations.</p>

<p>Let’s start by the lemma’s statement:</p>

<div class="theorem">
    <h4>Farkas' lemma</h4>
    For a matrix \(A \in \mathbb{R}^{m \times n}\) and \(b \in \mathbb{R}^{m}\) one and only one of these two things are true:<br />
    - There exists \(x \in \mathbb{R}^{n}\) such that \(Ax = b\) and \(x \geq 0\) <br />
    - There exists \(y \in \mathbb{R}^{m}\) such that \((A^T)y \geq 0\) and \((b^T)y &lt; 0\)
</div>

<p>The part of this theorem that may be unusual is the fact that we put an order on our vector space. Here when we say \(x \leq 0\) we mean that each component of \(x\) is less or equal than zero.</p>

<h2 id="the-first-part-of-the-alternative">The first part of the alternative</h2>

<p>For illustration purposes, we will place ourselves in \(\mathbb{R}^2\) with a matrix 
\(\begin{pmatrix}
1 &amp; 3\\
2 &amp; 1
\end{pmatrix}\)
We will denote \(v_1\) and \(v_2\) the columns of this matrix.</p>

<p>With this order defined, the first statement can be rephrased by saying “\(b\) is the image of a point \(x\) in the first quadrant of the plane”. Let’s have a look at how \(A\) transforms the first quadrant of the plane by applying it to points with positive coordinates.
<br />
Move the slider to operate the transformation, you might need to play with it a little before it becomes smooth enough.
<!-- Animation 1 --></p>
<div id="image-container-1">
    <img id="animation-image-1" src="/assets/farkas/shear_frames/frame0001.jpg" alt="Animation Frame 1" />
</div>
<p><input type="range" id="slider-1" min="1" max="300" value="1" step="1" /></p>

<p>We can see that we obtain a <em>cone</em> that we will call \(S\).</p>

<video id="autoplay-video" width="600" muted="" playsinline="">
    <source src="/assets/farkas/cone.mp4" type="video/mp4" />
    Your browser does not support the video tag.
</video>

<p>That’s all there is to the first statement, our first choice is that \(x\) is in the cone defined by \(A\). So what’s the alternative?</p>

<h2 id="the-second-part-of-the-alternative">The second part of the alternative</h2>

<p>First, we need to have a clear intuition of what a dot product does, that is tell us if two vectors are “pointing in the same direction”. In \(\mathbb{R}^2\) saying that \(u \cdot v = u^T v &lt; 0\) means that if we consider the plane \(P\) orthogonal to \(u\), then \(u\) and \(v\) are on opposite sides of \(P\).<br />
With that being said, let’s look at the conditions in this second part:<br />
\((b^T)y &lt; 0\): this means that \(b\) and \(y\) have an angle between them that is above \(\frac{\pi}{2}\)<br />
\((A^T)y \geq 0\): the product \((A^T)y\) is none other than the vector
\(\begin{pmatrix}
v_{1} \cdot y\\
v_{2} \cdot y
\end{pmatrix}\),
which is positive when \(y\) “faces the same direction” as both \(v_1\) and \(v_2\). 
(Visually, this means that \(y\) is in both of the half planes colored in white in the animation you will find a little further down below.)
<br />
The point of this post is not to go into detail about how one might prove this, see <a href="https://people.orie.cornell.edu/dpw/orie6300/fall2008/Lectures/lec07.pdf">this page</a> for a rigorous proof. However, we’ll outline the main idea: it relies on the <a href="https://en.wikipedia.org/wiki/Hyperplane_separation_theorem">Hyperplane separation theorem</a> in Euclidean space. <br /></p>

<p>The idea is that, no matter how close \(b\) is to our cone \(S\), as long as it is not in it, we can squeeze a hyperplane (in our case, a line) between \(b\) and \(S\). (This relies on the fact that \(S\) is closed and convex and \({b}\) is compact.)
This hyperplane is defined by a vector \(y\) that verifies \(y \cdot s &lt; 0\) for all \(s \in S\)
<!-- Animation 2 --></p>
<div id="image-container-2">
    <img id="animation-image-2" src="/assets/farkas/hp_frames/frame0001.jpg" alt="Animation Frame 2" />
</div>
<p><input type="range" id="slider-2" min="1" max="300" value="1" step="1" /></p>

<p>It comes pretty naturally that picking \(-y\) gives us a natural candidate for our alternative (the \(y\) in our alternative is our \(-y\) here, I did not think this through beforehand).
<!-- Animation 3 --></p>
<div id="image-container-3">
    <img id="animation-image-3" src="/assets/farkas/flippedy_frames/frame0001.jpg" alt="Animation Frame 3" />
</div>
<p><input type="range" id="slider-3" min="1" max="200" value="1" step="1" /></p>

<p>I hope this little post has made this lemma a little clearer for some of you! If you wish to know how the “interactive” animations were created, I’ll explain it here. I mut say that most of the HTML and Javascript was written by ChatGPT, as webdev is something I have no intention of learning anytime soon:</p>

<h2 id="step-one-create-animations-using-manim">Step one: create animations using Manim</h2>

<p>As I said earlier, the animations were made using <a href="https://www.manim.community/">Manim</a>. So go ahead and create your animation, keep in mind that this little “hack” is restricted to animations in “one dimension”, which is why they work with a slider. Export your scenes as short mp4 files with a reasonably high framerate.</p>

<h2 id="step-two-chop-the-images">Step two: chop the images</h2>

<p>Using <a href="https://www.ffmpeg.org/">ffmpeg</a>, save the frames of your video by running <code class="language-plaintext highlighter-rouge">ffmpeg -i your_video.mp4 -vf "fps=30" frames/frame%04d.jpg</code> this will save all of the frames in your video to a dedicated folder.</p>

<h2 id="step-three-embed-the-images-and-add-a-slider">Step three: embed the images and add a slider</h2>

<p>For this step, I’m just gonna give you the code. Just make sure that you replace the parameters in the function call at the end</p>
<h3 id="javascript-code">Javascript code</h3>

<pre><code class="language-javascript">
function initFrameAnimation(framePath, totalFrames, imageElementId, sliderElementId) {
        const animationImage = document.getElementById(imageElementId);
        const slider = document.getElementById(sliderElementId);

        if (!animationImage || !slider) {
            console.error('Invalid element IDs provided.');
            return;
        }

        slider.min = 1;
        slider.max = totalFrames;
        slider.value = 1;
        slider.step = 1;

        function getFrameSrc(frameNumber) {
            const frameString = String(frameNumber).padStart(4, '0');
            return framePath + frameString + '.jpg';
        }

        slider.addEventListener('input', function() {
            const frameNumber = parseInt(slider.value);
            animationImage.src = getFrameSrc(frameNumber);
        });

        function preloadImages(start, end) {
            for (let i = start; i &lt;= end; i++) {
                const img = new Image();
                img.src = getFrameSrc(i);
            }
        }

        preloadImages(1, Math.min(totalFrames, 10));
    }

    // Initialize animations
    initFrameAnimation('/assets/your_video/frame', number_of_frames_in_your_vid, 'animation-image-1', 'slider-1');
</code></pre>

<!-- JavaScript Code for the interactions-->
<script>
    /**
     * Function to initialize the frame-by-frame animation with a slider.
     */
    function initFrameAnimation(framePath, totalFrames, imageElementId, sliderElementId) {
        const animationImage = document.getElementById(imageElementId);
        const slider = document.getElementById(sliderElementId);

        if (!animationImage || !slider) {
            console.error('Invalid element IDs provided.');
            return;
        }

        slider.min = 1;
        slider.max = totalFrames;
        slider.value = 1;
        slider.step = 1;

        function getFrameSrc(frameNumber) {
            const frameString = String(frameNumber).padStart(4, '0');
            return framePath + frameString + '.jpg';
        }

        slider.addEventListener('input', function() {
            const frameNumber = parseInt(slider.value);
            animationImage.src = getFrameSrc(frameNumber);
        });

        function preloadImages(start, end) {
            for (let i = start; i <= end; i++) {
                const img = new Image();
                img.src = getFrameSrc(i);
            }
        }

        preloadImages(1, Math.min(totalFrames, 10));
    }

    // Initialize animations
    initFrameAnimation('/assets/farkas/flippedy_frames/frame', 52, 'animation-image-3', 'slider-3');
    initFrameAnimation('/assets/farkas/hp_frames/frame', 50, 'animation-image-2', 'slider-2');
    initFrameAnimation('/assets/farkas/shear_frames/frame', 95, 'animation-image-1', 'slider-1');
</script>

<!-- JavaScript Code for the videos autoplaying-->
<script>
    document.addEventListener("DOMContentLoaded", function() {
        const video = document.getElementById("autoplay-video");

        // Ensure IntersectionObserver is supported
        if ("IntersectionObserver" in window) {
            const observer = new IntersectionObserver((entries) => {
                entries.forEach((entry) => {
                    if (entry.isIntersecting) {
                        video.play();
                    } else {
                        video.pause();
                    }
                });
            }, { threshold: 0.5 }); // Trigger when 50% of the video is in view

            observer.observe(video);
        } else {
            // Fallback for browsers without IntersectionObserver support
            window.addEventListener("scroll", function() {
                const rect = video.getBoundingClientRect();
                if (rect.top < window.innerHeight && rect.bottom > 0) {
                    video.play();
                } else {
                    video.pause();
                }
            });
        }
    });
</script>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry></feed>